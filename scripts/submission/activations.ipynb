{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import HookedSAETransformer\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_name = 'gemma-2b'\n",
    "device = 'cuda'\n",
    "layer = 14\n",
    "hook_name = 'blocks.13.hook_resid_pre'\n",
    "max_length = 1024\n",
    "full_sequence_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(n_texts):\n",
    "    data = datasets.load_dataset('NeelNanda/pile-10k')['train']\n",
    "    texts = []\n",
    "\n",
    "    for i, t in enumerate(data):\n",
    "        if len(t['text']) > 6000:\n",
    "            texts.append(t['text'])\n",
    "\n",
    "        if len(texts) == n_texts:\n",
    "            break\n",
    "    \n",
    "    if len(texts) < n_texts:\n",
    "        raise ValueError('not enough texts, only found', len(texts))\n",
    "\n",
    "\n",
    "    print('went through', i, 'texts')\n",
    "\n",
    "    return texts\n",
    "\n",
    "def save_acts(transformer_name, device, layer, hook_name, max_length, full_sequence_batch_size):\n",
    "    transformer = HookedSAETransformer.from_pretrained(transformer_name, device=device)\n",
    "    transformer.eval()\n",
    "    tok = transformer.tokenizer\n",
    "\n",
    "    texts = get_texts(100)\n",
    "    input = tok(texts, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "\n",
    "    sequence_acts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, input.input_ids.shape[0], full_sequence_batch_size):\n",
    "            batch = input.input_ids[i:i+full_sequence_batch_size].to(device)\n",
    "            acts = transformer.run_with_cache(batch, prepend_bos=False, stop_at_layer=layer)[1][hook_name]\n",
    "\n",
    "            for j in range(batch.shape[0]):\n",
    "                sequence_acts.append(acts[j].to('cpu'))\n",
    "\n",
    "    sequence_acts = torch.stack(sequence_acts)\n",
    "\n",
    "    torch.save(sequence_acts, f'../../cruft{transformer_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e637f7f499445383ac27952ab876b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n",
      "went through 704 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a44fd6b81146c6a5afd7c973c8a7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-9b into HookedTransformer\n",
      "went through 704 texts\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "went through 704 texts\n"
     ]
    }
   ],
   "source": [
    "save_acts(\n",
    "    transformer_name='gemma-2b',\n",
    "    device='cuda',\n",
    "    layer=14,\n",
    "    hook_name='blocks.13.hook_resid_pre',\n",
    "    max_length=1024,\n",
    "    full_sequence_batch_size=4\n",
    ")\n",
    "\n",
    "\n",
    "save_acts(\n",
    "    transformer_name='gemma-2-9b',\n",
    "    device='cuda',\n",
    "    layer=34,\n",
    "    hook_name='blocks.33.hook_resid_pre',\n",
    "    max_length=1024,\n",
    "    full_sequence_batch_size=1\n",
    ")\n",
    "\n",
    "save_acts(\n",
    "    transformer_name='gpt2',\n",
    "    device='cuda',\n",
    "    layer=11,\n",
    "    hook_name='blocks.10.hook_resid_pre',\n",
    "    max_length=1024,\n",
    "    full_sequence_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.mean(acts, dim=0)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(2048):\n",
    "    plt.plot(a[:10, i], alpha=0.05)\n",
    "\n",
    "plt.ylim(-15, 15)\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
