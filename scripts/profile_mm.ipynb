{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "\n",
    "d_input =  768\n",
    "n_ft = 24576\n",
    "trials = 10\n",
    "warm_up = 10\n",
    "\n",
    "device='cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_mm(w, input):\n",
    "    y = input @ w\n",
    "    return y\n",
    "\n",
    "def expert_mm(experts, bs, input):\n",
    "    n_experts = experts.shape[0]\n",
    "    to_each_expert = bs // n_experts\n",
    "\n",
    "    for i in range(0, bs, to_each_expert):\n",
    "        y = input[list(range(i, to_each_expert * 2, 2)), :] @ experts[i // to_each_expert]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experts = 32\n",
    "bs = 1024\n",
    "weights = torch.randn(d_input, n_ft, device=device)\n",
    "experts = torch.randn(n_experts, d_input, n_ft // n_experts, device=device)\n",
    "input = torch.randn(bs, d_input, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "             aten::matmul         0.17%      12.000us        19.27%       1.324ms       1.324ms       0.000us         0.00%       5.613ms       5.613ms             1  \n",
      "                 aten::mm        10.95%     752.000us        19.10%       1.312ms       1.312ms       5.613ms       100.00%       5.613ms       5.613ms             1  \n",
      "    volta_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us       5.613ms       100.00%       5.613ms       5.613ms             1  \n",
      "         cudaLaunchKernel         8.15%     560.000us         8.15%     560.000us     560.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        80.73%       5.546ms        80.73%       5.546ms       5.546ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 6.870ms\n",
      "Self CUDA time total: 5.613ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-08-18 01:21:09 39506:39506 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-08-18 01:21:10 39506:39506 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-08-18 01:21:10 39506:39506 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    normal_mm(weights, input)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         1.76%      38.000us        14.31%     308.000us       9.625us       0.000us         0.00%      33.000us       1.031us            32  \n",
      "                                               aten::mm        10.92%     235.000us        12.54%     270.000us       8.438us      33.000us        76.74%      33.000us       1.031us            32  \n",
      "                                  volta_sgemm_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us      16.000us        37.21%      16.000us      16.000us             1  \n",
      "                         volta_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us        32.56%      14.000us      14.000us             1  \n",
      "                                            aten::index        20.72%     446.000us        24.57%     529.000us      16.531us      10.000us        23.26%      10.000us       0.312us            32  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us        23.26%      10.000us       5.000us             2  \n",
      "void splitKreduce_kernel<32, 16, int, float, float, ...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         6.98%       3.000us       1.500us             2  \n",
      "                                            aten::empty         0.88%      19.000us         0.88%      19.000us       0.594us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                               aten::to         1.81%      39.000us        48.21%       1.038ms      32.438us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "                                         aten::_to_copy         6.69%     144.000us        46.40%     999.000us      31.219us       0.000us         0.00%       0.000us       0.000us            32  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.153ms\n",
      "Self CUDA time total: 43.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-08-18 01:21:10 39506:39506 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-08-18 01:21:10 39506:39506 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-08-18 01:21:10 39506:39506 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    expert_mm(experts, bs, input)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "bss = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "n_expertss = [16]\n",
    "n_threadss = [1, 4, 8]\n",
    "label = 'mm'\n",
    "results = []\n",
    "for bs in bss:\n",
    "    for n_experts in n_expertss:\n",
    "        for n_threads in n_threadss:\n",
    "            weights = torch.randn(d_input, n_ft, device=device)\n",
    "            experts = torch.randn(n_experts, d_input, n_ft // n_experts, device=device)\n",
    "            input = torch.randn(bs, d_input, device=device)\n",
    "\n",
    "\n",
    "            sub_label = f'bs={bs}, n_experts={n_experts}'\n",
    "            t0 = benchmark.Timer(\n",
    "                stmt='normal_mm(weights, input)',\n",
    "                setup='from __main__ import normal_mm',\n",
    "                description='normal',\n",
    "                num_threads=n_threads,\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                globals={'input': input, 'weights': weights}).blocked_autorange(min_run_time=1)\n",
    "\n",
    "            t1 = benchmark.Timer(\n",
    "                stmt='expert_mm(experts, bs, input)',\n",
    "                setup='from __main__ import expert_mm',\n",
    "                description='expert',\n",
    "                num_threads=n_threads,\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                globals={'input': input, 'experts': experts, 'bs': bs}).blocked_autorange(min_run_time=1)\n",
    "\n",
    "            results.append(t0)\n",
    "            results.append(t1)\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "bss = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "n_expertss = [8, 16, 32]\n",
    "for bs in bss:\n",
    "    for n_experts in n_expertss:\n",
    "        weights = torch.randn(d_input, n_ft, device=device)\n",
    "        experts = torch.randn(n_experts, d_input, n_ft // n_experts, device=device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(trials):\n",
    "            y = normal_mm(weights, bs, d_input)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_a = time.time() - start\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(trials):\n",
    "            ys = expert_mm(experts, bs, d_input)\n",
    "                \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_b = time.time() - start\n",
    "\n",
    "        results.append({\n",
    "            'bs': bs,\n",
    "            'n_experts': n_experts,\n",
    "            'elapsed_normal': elapsed_a,\n",
    "            'elapsed_experts': elapsed_b\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_normal = []\n",
    "all_experts = []\n",
    "\n",
    "for r in results:\n",
    "    all_normal.append(r['elapsed_normal'])\n",
    "    all_experts.append(r['elapsed_experts'])\n",
    "\n",
    "plt.plot(all_normal, label='normal')\n",
    "plt.plot(all_experts, label='experts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = sorted(set(r['bs'] for r in results))\n",
    "num_experts = sorted(set(r['n_experts'] for r in results))\n",
    "elapsed_normal = np.zeros((len(batch_sizes), len(num_experts)))\n",
    "elapsed_experts = np.zeros((len(batch_sizes), len(num_experts)))\n",
    "\n",
    "for r in results:\n",
    "    bs_idx = batch_sizes.index(r['bs'])\n",
    "    n_exp_idx = num_experts.index(r['n_experts'])\n",
    "    elapsed_normal[bs_idx, n_exp_idx] = r['elapsed_normal']\n",
    "    elapsed_experts[bs_idx, n_exp_idx] = r['elapsed_experts']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Normal settings plot\n",
    "for idx, bs in enumerate(batch_sizes):\n",
    "    ax[0].bar(np.arange(len(num_experts)) + idx * 0.1, elapsed_normal[idx], width=0.1,\n",
    "              label=f'Batch Size {bs}')\n",
    "ax[0].set_xticks(np.arange(len(num_experts)) + 0.1 * (len(batch_sizes) - 1) / 2)\n",
    "ax[0].set_xticklabels([f'{ne} Experts' for ne in num_experts])\n",
    "ax[0].set_ylabel('Elapsed Time (s)')\n",
    "ax[0].set_title('Elapsed Time for Normal Setting')\n",
    "ax[0].legend()\n",
    "\n",
    "# Experts settings plot\n",
    "for idx, bs in enumerate(batch_sizes):\n",
    "    ax[1].bar(np.arange(len(num_experts)) + idx * 0.1, elapsed_experts[idx], width=0.1,\n",
    "              label=f'Batch Size {bs}')\n",
    "ax[1].set_xticks(np.arange(len(num_experts)) + 0.1 * (len(batch_sizes) - 1) / 2)\n",
    "ax[1].set_xticklabels([f'{ne} Experts' for ne in num_experts])\n",
    "ax[1].set_ylabel('Elapsed Time (s)')\n",
    "ax[1].set_title('Elapsed Time for Expert Setting')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.xlabel('Number of Experts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
