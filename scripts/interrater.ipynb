{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "def load_predictions(predictions_file):\n",
    "    \"\"\"\n",
    "    Load predictions from the first JSON file.\n",
    "    Returns a dictionary with keys as (idx1, idx2) tuples and values as prediction indices.\n",
    "    \"\"\"\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    pred_dict = {}\n",
    "    for entry in predictions:\n",
    "        key = (entry['idx1'], entry['idx2'])\n",
    "        pred_dict[key] = entry['prediction']\n",
    "    \n",
    "    return pred_dict\n",
    "\n",
    "def load_evaluations(evaluations_file):\n",
    "    \"\"\"\n",
    "    Load evaluations from the second JSON file.\n",
    "    Returns a dictionary with keys as (feature1_idx, feature2_idx) tuples and values as evaluation details.\n",
    "    \"\"\"\n",
    "    with open(evaluations_file, 'r') as f:\n",
    "        evaluations = json.load(f)\n",
    "    \n",
    "    eval_dict = {}\n",
    "    for pair_key, details in evaluations['evaluations'].items():\n",
    "        key = (details['feature1_idx'], details['feature2_idx'])\n",
    "        eval_dict[key] = details\n",
    "    \n",
    "    return eval_dict\n",
    "\n",
    "def map_rater1_predictions(pred_dict, eval_dict):\n",
    "    \"\"\"\n",
    "    Map Rater 1's predictions to selection numbers (1 or 2) based on feature indices.\n",
    "    Returns a dictionary with keys as pair tuples and values as 1 or 2.\n",
    "    \"\"\"\n",
    "    rater1_selection = {}\n",
    "    for key, pred in pred_dict.items():\n",
    "        if key in eval_dict:\n",
    "            feature1, feature2 = key\n",
    "            if pred == feature1:\n",
    "                rater1_selection[key] = 1\n",
    "            elif pred == feature2:\n",
    "                rater1_selection[key] = 2\n",
    "            else:\n",
    "                # Handle cases where prediction doesn't match either feature\n",
    "                # Assign None or handle as per requirements\n",
    "                rater1_selection[key] = None\n",
    "        else:\n",
    "            # Pair not found in evaluations; skip or handle accordingly\n",
    "            pass\n",
    "    return rater1_selection\n",
    "\n",
    "def map_rater2_predictions(eval_dict):\n",
    "    \"\"\"\n",
    "    Infer Rater 2's selections based on the 'correct' flag.\n",
    "    If 'correct' is True, Rater 2 selected 'correct_index'.\n",
    "    If 'correct' is False, Rater 2 selected the other index.\n",
    "    Returns a dictionary with keys as pair tuples and values as 1 or 2.\n",
    "    \"\"\"\n",
    "    rater2_selection = {}\n",
    "    for key, details in eval_dict.items():\n",
    "        feature1, feature2 = key\n",
    "        correct_idx = details['correct_index']\n",
    "        if details['correct']:\n",
    "            selected_feature = correct_idx\n",
    "        else:\n",
    "            # Select the other index\n",
    "            selected_feature = feature2 if correct_idx == feature1 else feature1\n",
    "        \n",
    "        if selected_feature == feature1:\n",
    "            rater2_selection[key] = 1\n",
    "        elif selected_feature == feature2:\n",
    "            rater2_selection[key] = 2\n",
    "        else:\n",
    "            # Handle unexpected cases\n",
    "            rater2_selection[key] = None\n",
    "    return rater2_selection\n",
    "\n",
    "def compute_inter_rater_reliability(r1, r2):\n",
    "    \"\"\"\n",
    "    Compute Cohen's Kappa for inter-rater reliability.\n",
    "    \"\"\"\n",
    "    # Ensure both raters have the same set of pairs\n",
    "    common_keys = set(r1.keys()).intersection(set(r2.keys()))\n",
    "    r1_labels = []\n",
    "    r2_labels = []\n",
    "    for key in common_keys:\n",
    "        if r1[key] is not None and r2[key] is not None:\n",
    "            r1_labels.append(r1[key])\n",
    "            r2_labels.append(r2[key])\n",
    "    \n",
    "    if not r1_labels:\n",
    "        print(\"No overlapping valid predictions to compare.\")\n",
    "        return None\n",
    "    \n",
    "    kappa = cohen_kappa_score(r1_labels, r2_labels)\n",
    "    return kappa\n",
    "\n",
    "def compute_accuracy(r1, r2, eval_dict):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of both raters against the ground truth.\n",
    "    Returns a tuple (r1_accuracy, r2_accuracy).\n",
    "    \"\"\"\n",
    "    r1_correct = []\n",
    "    r2_correct = []\n",
    "    r1_labels = []\n",
    "    r2_labels = []\n",
    "    \n",
    "    for key, details in eval_dict.items():\n",
    "        # Rater 1\n",
    "        if key in r1 and r1[key] is not None:\n",
    "            r1_prediction_idx = key[0] if r1[key] == 1 else key[1]\n",
    "            is_correct = int(r1_prediction_idx == details['correct_index'])\n",
    "            r1_labels.append(is_correct)\n",
    "        \n",
    "        # Rater 2\n",
    "        if key in r1 and r2[key] is not None:\n",
    "            r2_prediction_idx = key[0] if r2[key] == 1 else key[1]\n",
    "            is_correct = int(r2_prediction_idx == details['correct_index'])\n",
    "            r2_labels.append(is_correct)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    print('total human labels', len(r1_labels))\n",
    "    print('total gpt labels', len(r2_labels))\n",
    "    r1_accuracy = (sum(r1_labels) / len(r1_labels)) if r1_labels else 0\n",
    "    r2_accuracy = (sum(r2_labels) / len(r2_labels)) if r2_labels else 0\n",
    "    \n",
    "    return r1_accuracy, r2_accuracy\n",
    "\n",
    "def evaluate_interrater(pred_file, eval_file):\n",
    "    # Load JSON files\n",
    "    pred_dict = load_predictions(pred_file)\n",
    "    eval_dict = load_evaluations(eval_file)\n",
    "    \n",
    "    # Map predictions to selections\n",
    "    rater1 = map_rater1_predictions(pred_dict, eval_dict)\n",
    "    rater2 = map_rater2_predictions(eval_dict)\n",
    "    \n",
    "    # Compute Inter-Rater Reliability\n",
    "    kappa = compute_inter_rater_reliability(rater1, rater2)\n",
    "    if kappa is not None:\n",
    "        print(f\"Inter-Rater Reliability (Cohen's Kappa): {kappa:.4f}\")\n",
    "    else:\n",
    "        print(\"Unable to compute Cohen's Kappa due to lack of overlapping valid predictions.\")\n",
    "    \n",
    "    # Compute Accuracy\n",
    "    r1_acc, r2_acc = compute_accuracy(rater1, rater2, eval_dict)\n",
    "    print(f\"Human Accuracy: {r1_acc*100:.2f}%\")\n",
    "    print(f\"GPT-4o Accuracy: {r2_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Rater Reliability (Cohen's Kappa): 0.8135\n",
      "total human labels 35\n",
      "total gpt labels 35\n",
      "Human Accuracy: 91.43%\n",
      "GPT-4o Accuracy: 88.57%\n"
     ]
    }
   ],
   "source": [
    "pred_file = '../cruft/ViT-3mil-topkk-32-experts-None_1aaa89/latents-2969600/images/predictions.json'\n",
    "eval_file = '../cruft/ViT-3mil-topkk-32-experts-None_1aaa89/latents-2969600/images/gpt4_evaluations.json'\n",
    "\n",
    "evaluate_interrater(pred_file, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Rater Reliability (Cohen's Kappa): 0.8264\n",
      "total human labels 35\n",
      "total gpt labels 35\n",
      "Human Accuracy: 94.29%\n",
      "GPT-4o Accuracy: 97.14%\n"
     ]
    }
   ],
   "source": [
    "pred_file = '../cruft/ViT-3mil-topkk-32-experts-8_5d073c/latents-2969600/images/predictions.json'\n",
    "eval_file = '../cruft/ViT-3mil-topkk-32-experts-8_5d073c/latents-2969600/images/gpt4_evaluations.json'\n",
    "\n",
    "evaluate_interrater(pred_file, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1380679892.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    aws s3 sync ./cruft/ViT-3mil-topkk-32-experts-None_1aaa89 s3://vit-sae-switch/images/ViT-3mil-topkk-32-experts-None_1aaa89\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# aws s3 sync ./cruft/ViT-3mil-topkk-32-experts-None_1aaa89 s3://vit-sae-switch/images/ViT-3mil-topkk-32-experts-None_1aaa89\n",
    "# aws s3 sync ./cruft/ViT-3mil-topkk-32-experts-8_5d073c s3://vit-sae-switch/images/ViT-3mil-topkk-32-experts-8_5d073c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
